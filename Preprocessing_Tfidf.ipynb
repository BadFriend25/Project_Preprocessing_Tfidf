{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29dc084c-9c3b-4d59-98e0-80ae51ab5f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most unique words in the PDF file (excluding stop words):\n",
      "pasukan: 0.3307\n",
      "liu: 0.2384\n",
      "caocao: 0.1966\n",
      "zhang: 0.1682\n",
      "bei: 0.1615\n",
      "zhuge: 0.1361\n",
      "wei: 0.1289\n",
      "liang: 0.1257\n",
      "pergi: 0.1254\n",
      "orang: 0.1251\n",
      "\n",
      "Bottom 10 least unique words in the PDF file (excluding stop words):\n",
      "14tahun: 0.0001\n",
      "110: 0.0001\n",
      "104: 0.0001\n",
      "100x: 0.0001\n",
      "100an: 0.0001\n",
      "10001500: 0.0001\n",
      "08: 0.0001\n",
      "07: 0.0001\n",
      "00000400000: 0.0001\n",
      "000000: 0.0001\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def get_all_words_in_pdf(pdf_path, stop_words=None):\n",
    "    with open(pdf_path, 'rb') as f:\n",
    "        pdf_reader = PyPDF2.PdfReader(f)\n",
    "        text = \"\"\n",
    "        for page in pdf_reader.pages:\n",
    "            text += page.extract_text()\n",
    "\n",
    "    # Tokenize text into words\n",
    "    words = text.split()\n",
    "\n",
    "    # Remove punctuation\n",
    "    words_no_punct = [word.translate(str.maketrans('', '', string.punctuation)) for word in words]\n",
    "\n",
    "    # Create a TF-IDF vectorizer with custom stop words\n",
    "    vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
    "    tfidf = vectorizer.fit_transform([' '.join(words_no_punct)])\n",
    "\n",
    "    # Get the feature names (unique words) and their TF-IDF scores\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    tfidf_scores = tfidf.toarray().sum(axis=0)\n",
    "\n",
    "    return feature_names, tfidf_scores\n",
    "\n",
    "# Specify custom stop words in Indonesian\n",
    "stop_words_complete = ['dan', 'saya', 'ini', 'itu', 'apa', 'siapa', 'kapan', 'di', 'mana', 'yang', 'dia', 'untuk', 'mereka', 'aku', 'tidak', 'akan', 'segera', 'dengan', 'kau', 'dari', 'telah', 'tetapi', 'berkata', 'kita', 'dapat', 'pada', 'adalah', 'lalu', 'ketika', 'kemudian', 'bahwa', 'ada', 'sangat', 'jika', 'ke', 'hal', 'juga', 'karena', 'maka', 'lagi', 'seperti', 'itu', 'saja']\n",
    "\n",
    "pdf_path = './Samkok - Luo Guanzhong.pdf'\n",
    "all_words, tfidf_scores = get_all_words_in_pdf(pdf_path, stop_words_complete)\n",
    "\n",
    "# Sort words by their TF-IDF scores in descending order\n",
    "sorted_words = sorted(zip(tfidf_scores, all_words), reverse=True)\n",
    "\n",
    "print(\"Top 10 most unique words in the PDF file (excluding stop words):\")\n",
    "for score, word in sorted_words[:10]:\n",
    "    print(f\"{word}: {score:.4f}\")\n",
    "\n",
    "print(\"\\nBottom 10 least unique words in the PDF file (excluding stop words):\")\n",
    "for score, word in sorted_words[-10:]:\n",
    "    print(f\"{word}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9432b88-8544-4817-ac61-36d240c35124",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 31) (3797062493.py, line 31)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[5], line 31\u001b[1;36m\u001b[0m\n\u001b[1;33m    pdf_path = './Kementerian Kominfo_Situs Elaelo Pengganti X_Twitter Bukan Buatan Pemerintah.pdf\"\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unterminated string literal (detected at line 31)\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def get_all_words_in_pdf(pdf_path, stop_words=None):\n",
    "    with open(pdf_path, 'rb') as f:\n",
    "        pdf_reader = PyPDF2.PdfReader(f)\n",
    "        text = \"\"\n",
    "        for page in pdf_reader.pages:\n",
    "            text += page.extract_text()\n",
    "\n",
    "    # Tokenize text into words\n",
    "    words = text.split()\n",
    "\n",
    "    # Remove punctuation\n",
    "    words_no_punct = [word.translate(str.maketrans('', '', string.punctuation)) for word in words]\n",
    "\n",
    "    # Create a TF-IDF vectorizer with custom stop words\n",
    "    vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
    "    tfidf = vectorizer.fit_transform([' '.join(words_no_punct)])\n",
    "\n",
    "    # Get the feature names (unique words) and their TF-IDF scores\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    tfidf_scores = tfidf.toarray().sum(axis=0)\n",
    "\n",
    "    return feature_names, tfidf_scores\n",
    "\n",
    "# Specify custom stop words in Indonesian\n",
    "stop_words_complete = ['dan', 'saya', 'ini', 'itu', 'apa', 'siapa', 'kapan', 'di', 'mana', 'yang', 'dia', 'untuk', 'mereka', 'aku', 'tidak', 'akan', 'segera', 'dengan', 'kau', 'dari', 'telah', 'tetapi', 'berkata', 'kita', 'dapat', 'pada', 'adalah', 'lalu', 'ketika', 'kemudian', 'bahwa', 'ada', 'sangat', 'jika', 'ke', 'hal', 'juga', 'karena', 'maka', 'lagi', 'seperti', 'itu', 'saja']\n",
    "\n",
    "pdf_path = './Kementerian Kominfo_Situs Elaelo Pengganti X_Twitter Bukan Buatan Pemerintah.pdf\"\n",
    "all_words, tfidf_scores = get_all_words_in_pdf(pdf_path, stop_words_complete)\n",
    "\n",
    "# Sort words by their TF-IDF scores in descending order\n",
    "sorted_words = sorted(zip(tfidf_scores, all_words), reverse=True)\n",
    "\n",
    "print(\"Top 10 most unique words in the PDF file (excluding stop words):\")\n",
    "for score, word in sorted_words[:10]:\n",
    "    print(f\"{word}: {score:.4f}\")\n",
    "\n",
    "print(\"\\nBottom 10 least unique words in the PDF file (excluding stop words):\")\n",
    "for score, word in sorted_words[-10:]:\n",
    "    print(f\"{word}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64a07e3-7d4c-43f6-9d72-6e3717cb65ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
